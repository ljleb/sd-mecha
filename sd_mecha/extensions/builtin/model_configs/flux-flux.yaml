identifier: flux_dev-flux
components:
  diffuser:
    model.diffusion_model.double_blocks.0.img_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.0.img_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.0.img_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.0.img_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.0.img_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.0.img_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.0.img_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.0.img_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.0.img_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.0.img_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.0.img_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.0.img_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.0.txt_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.0.txt_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.0.txt_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.0.txt_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.0.txt_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.0.txt_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.0.txt_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.0.txt_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.0.txt_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.0.txt_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.0.txt_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.0.txt_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.img_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.img_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.img_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.img_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.img_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.img_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.img_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.img_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.img_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.img_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.img_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.img_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.txt_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.txt_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.txt_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.txt_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.txt_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.txt_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.txt_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.txt_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.txt_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.txt_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.txt_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.1.txt_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.img_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.img_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.img_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.img_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.img_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.img_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.img_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.img_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.img_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.img_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.img_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.img_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.txt_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.txt_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.txt_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.txt_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.txt_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.txt_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.txt_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.txt_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.txt_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.txt_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.txt_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.10.txt_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.img_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.img_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.img_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.img_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.img_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.img_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.img_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.img_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.img_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.img_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.img_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.img_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.txt_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.txt_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.txt_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.txt_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.txt_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.txt_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.txt_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.txt_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.txt_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.txt_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.txt_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.11.txt_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.img_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.img_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.img_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.img_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.img_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.img_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.img_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.img_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.img_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.img_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.img_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.img_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.txt_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.txt_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.txt_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.txt_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.txt_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.txt_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.txt_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.txt_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.txt_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.txt_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.txt_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.12.txt_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.img_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.img_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.img_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.img_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.img_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.img_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.img_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.img_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.img_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.img_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.img_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.img_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.txt_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.txt_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.txt_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.txt_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.txt_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.txt_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.txt_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.txt_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.txt_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.txt_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.txt_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.13.txt_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.img_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.img_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.img_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.img_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.img_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.img_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.img_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.img_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.img_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.img_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.img_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.img_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.txt_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.txt_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.txt_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.txt_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.txt_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.txt_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.txt_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.txt_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.txt_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.txt_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.txt_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.14.txt_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.img_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.img_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.img_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.img_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.img_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.img_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.img_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.img_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.img_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.img_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.img_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.img_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.txt_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.txt_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.txt_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.txt_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.txt_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.txt_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.txt_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.txt_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.txt_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.txt_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.txt_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.15.txt_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.img_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.img_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.img_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.img_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.img_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.img_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.img_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.img_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.img_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.img_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.img_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.img_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.txt_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.txt_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.txt_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.txt_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.txt_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.txt_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.txt_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.txt_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.txt_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.txt_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.txt_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.16.txt_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.img_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.img_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.img_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.img_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.img_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.img_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.img_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.img_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.img_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.img_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.img_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.img_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.txt_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.txt_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.txt_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.txt_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.txt_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.txt_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.txt_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.txt_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.txt_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.txt_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.txt_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.17.txt_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.img_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.img_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.img_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.img_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.img_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.img_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.img_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.img_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.img_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.img_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.img_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.img_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.txt_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.txt_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.txt_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.txt_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.txt_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.txt_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.txt_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.txt_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.txt_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.txt_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.txt_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.18.txt_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.img_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.img_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.img_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.img_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.img_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.img_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.img_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.img_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.img_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.img_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.img_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.img_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.txt_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.txt_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.txt_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.txt_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.txt_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.txt_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.txt_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.txt_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.txt_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.txt_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.txt_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.2.txt_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.img_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.img_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.img_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.img_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.img_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.img_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.img_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.img_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.img_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.img_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.img_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.img_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.txt_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.txt_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.txt_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.txt_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.txt_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.txt_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.txt_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.txt_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.txt_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.txt_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.txt_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.3.txt_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.img_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.img_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.img_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.img_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.img_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.img_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.img_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.img_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.img_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.img_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.img_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.img_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.txt_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.txt_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.txt_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.txt_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.txt_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.txt_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.txt_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.txt_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.txt_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.txt_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.txt_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.4.txt_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.img_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.img_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.img_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.img_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.img_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.img_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.img_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.img_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.img_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.img_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.img_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.img_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.txt_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.txt_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.txt_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.txt_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.txt_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.txt_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.txt_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.txt_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.txt_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.txt_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.txt_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.5.txt_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.img_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.img_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.img_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.img_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.img_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.img_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.img_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.img_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.img_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.img_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.img_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.img_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.txt_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.txt_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.txt_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.txt_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.txt_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.txt_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.txt_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.txt_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.txt_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.txt_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.txt_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.6.txt_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.img_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.img_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.img_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.img_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.img_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.img_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.img_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.img_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.img_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.img_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.img_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.img_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.txt_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.txt_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.txt_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.txt_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.txt_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.txt_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.txt_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.txt_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.txt_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.txt_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.txt_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.7.txt_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.img_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.img_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.img_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.img_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.img_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.img_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.img_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.img_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.img_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.img_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.img_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.img_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.txt_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.txt_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.txt_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.txt_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.txt_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.txt_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.txt_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.txt_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.txt_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.txt_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.txt_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.8.txt_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.img_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.img_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.img_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.img_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.img_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.img_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.img_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.img_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.img_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.img_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.img_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.img_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.txt_attn.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.txt_attn.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.txt_attn.proj.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.txt_attn.proj.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.txt_attn.qkv.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.txt_attn.qkv.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.txt_mlp.0.bias: {shape: [12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.txt_mlp.0.weight: {shape: [12288, 3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.txt_mlp.2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.txt_mlp.2.weight: {shape: [3072, 12288], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.txt_mod.lin.bias: {shape: [18432], dtype: bfloat16}
    model.diffusion_model.double_blocks.9.txt_mod.lin.weight: {shape: [18432, 3072], dtype: bfloat16}
    model.diffusion_model.final_layer.adaLN_modulation.1.bias: {shape: [6144], dtype: bfloat16}
    model.diffusion_model.final_layer.adaLN_modulation.1.weight: {shape: [6144, 3072], dtype: bfloat16}
    model.diffusion_model.final_layer.linear.bias: {shape: [64], dtype: bfloat16}
    model.diffusion_model.final_layer.linear.weight: {shape: [64, 3072], dtype: bfloat16}
    model.diffusion_model.guidance_in.in_layer.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.guidance_in.in_layer.weight: {shape: [3072, 256], dtype: bfloat16}
    model.diffusion_model.guidance_in.out_layer.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.guidance_in.out_layer.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.img_in.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.img_in.weight: {shape: [3072, 64], dtype: bfloat16}
    model.diffusion_model.single_blocks.0.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.0.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.0.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.0.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.0.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.0.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.0.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.0.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.1.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.1.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.1.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.1.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.1.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.1.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.1.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.1.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.10.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.10.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.10.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.10.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.10.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.10.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.10.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.10.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.11.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.11.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.11.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.11.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.11.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.11.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.11.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.11.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.12.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.12.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.12.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.12.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.12.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.12.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.12.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.12.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.13.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.13.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.13.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.13.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.13.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.13.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.13.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.13.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.14.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.14.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.14.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.14.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.14.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.14.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.14.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.14.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.15.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.15.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.15.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.15.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.15.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.15.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.15.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.15.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.16.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.16.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.16.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.16.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.16.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.16.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.16.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.16.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.17.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.17.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.17.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.17.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.17.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.17.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.17.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.17.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.18.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.18.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.18.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.18.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.18.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.18.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.18.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.18.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.19.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.19.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.19.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.19.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.19.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.19.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.19.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.19.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.2.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.2.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.2.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.2.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.2.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.2.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.2.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.2.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.20.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.20.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.20.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.20.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.20.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.20.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.20.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.20.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.21.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.21.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.21.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.21.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.21.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.21.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.21.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.21.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.22.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.22.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.22.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.22.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.22.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.22.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.22.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.22.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.23.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.23.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.23.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.23.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.23.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.23.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.23.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.23.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.24.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.24.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.24.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.24.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.24.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.24.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.24.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.24.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.25.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.25.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.25.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.25.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.25.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.25.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.25.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.25.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.26.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.26.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.26.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.26.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.26.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.26.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.26.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.26.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.27.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.27.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.27.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.27.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.27.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.27.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.27.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.27.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.28.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.28.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.28.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.28.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.28.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.28.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.28.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.28.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.29.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.29.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.29.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.29.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.29.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.29.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.29.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.29.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.3.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.3.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.3.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.3.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.3.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.3.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.3.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.3.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.30.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.30.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.30.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.30.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.30.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.30.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.30.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.30.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.31.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.31.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.31.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.31.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.31.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.31.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.31.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.31.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.32.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.32.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.32.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.32.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.32.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.32.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.32.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.32.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.33.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.33.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.33.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.33.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.33.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.33.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.33.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.33.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.34.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.34.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.34.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.34.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.34.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.34.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.34.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.34.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.35.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.35.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.35.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.35.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.35.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.35.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.35.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.35.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.36.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.36.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.36.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.36.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.36.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.36.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.36.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.36.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.37.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.37.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.37.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.37.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.37.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.37.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.37.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.37.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.4.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.4.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.4.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.4.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.4.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.4.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.4.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.4.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.5.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.5.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.5.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.5.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.5.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.5.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.5.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.5.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.6.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.6.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.6.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.6.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.6.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.6.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.6.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.6.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.7.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.7.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.7.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.7.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.7.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.7.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.7.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.7.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.8.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.8.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.8.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.8.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.8.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.8.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.8.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.8.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.9.linear1.bias: {shape: [21504], dtype: bfloat16}
    model.diffusion_model.single_blocks.9.linear1.weight: {shape: [21504, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.9.linear2.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.9.linear2.weight: {shape: [3072, 15360], dtype: bfloat16}
    model.diffusion_model.single_blocks.9.modulation.lin.bias: {shape: [9216], dtype: bfloat16}
    model.diffusion_model.single_blocks.9.modulation.lin.weight: {shape: [9216, 3072], dtype: bfloat16}
    model.diffusion_model.single_blocks.9.norm.key_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.single_blocks.9.norm.query_norm.scale: {shape: [128], dtype: bfloat16}
    model.diffusion_model.time_in.in_layer.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.time_in.in_layer.weight: {shape: [3072, 256], dtype: bfloat16}
    model.diffusion_model.time_in.out_layer.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.time_in.out_layer.weight: {shape: [3072, 3072], dtype: bfloat16}
    model.diffusion_model.txt_in.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.txt_in.weight: {shape: [3072, 4096], dtype: bfloat16}
    model.diffusion_model.vector_in.in_layer.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.vector_in.in_layer.weight: {shape: [3072, 768], dtype: bfloat16}
    model.diffusion_model.vector_in.out_layer.bias: {shape: [3072], dtype: bfloat16}
    model.diffusion_model.vector_in.out_layer.weight: {shape: [3072, 3072], dtype: bfloat16}
  clip_l:
    text_encoders.clip_l.transformer.text_model.embeddings.position_embedding.weight: {shape: [77, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.embeddings.token_embedding.weight: {shape: [49408, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.0.layer_norm1.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.0.layer_norm1.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.0.layer_norm2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.0.layer_norm2.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.0.mlp.fc1.bias: {shape: [3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.0.mlp.fc1.weight: {shape: [3072, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.0.mlp.fc2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.0.mlp.fc2.weight: {shape: [768, 3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.0.self_attn.k_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.0.self_attn.k_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.0.self_attn.out_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.0.self_attn.out_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.0.self_attn.q_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.0.self_attn.q_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.0.self_attn.v_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.0.self_attn.v_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.1.layer_norm1.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.1.layer_norm1.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.1.layer_norm2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.1.layer_norm2.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.1.mlp.fc1.bias: {shape: [3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.1.mlp.fc1.weight: {shape: [3072, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.1.mlp.fc2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.1.mlp.fc2.weight: {shape: [768, 3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.1.self_attn.k_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.1.self_attn.k_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.1.self_attn.out_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.1.self_attn.out_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.1.self_attn.q_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.1.self_attn.q_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.1.self_attn.v_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.1.self_attn.v_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.10.layer_norm1.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.10.layer_norm1.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.10.layer_norm2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.10.layer_norm2.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.10.mlp.fc1.bias: {shape: [3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.10.mlp.fc1.weight: {shape: [3072, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.10.mlp.fc2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.10.mlp.fc2.weight: {shape: [768, 3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.10.self_attn.k_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.10.self_attn.k_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.10.self_attn.out_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.10.self_attn.out_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.10.self_attn.q_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.10.self_attn.q_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.10.self_attn.v_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.10.self_attn.v_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.11.layer_norm1.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.11.layer_norm1.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.11.layer_norm2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.11.layer_norm2.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.11.mlp.fc1.bias: {shape: [3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.11.mlp.fc1.weight: {shape: [3072, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.11.mlp.fc2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.11.mlp.fc2.weight: {shape: [768, 3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.11.self_attn.k_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.11.self_attn.k_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.11.self_attn.out_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.11.self_attn.out_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.11.self_attn.q_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.11.self_attn.q_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.11.self_attn.v_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.11.self_attn.v_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.2.layer_norm1.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.2.layer_norm1.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.2.layer_norm2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.2.layer_norm2.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.2.mlp.fc1.bias: {shape: [3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.2.mlp.fc1.weight: {shape: [3072, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.2.mlp.fc2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.2.mlp.fc2.weight: {shape: [768, 3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.2.self_attn.k_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.2.self_attn.k_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.2.self_attn.out_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.2.self_attn.out_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.2.self_attn.q_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.2.self_attn.q_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.2.self_attn.v_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.2.self_attn.v_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.3.layer_norm1.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.3.layer_norm1.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.3.layer_norm2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.3.layer_norm2.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.3.mlp.fc1.bias: {shape: [3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.3.mlp.fc1.weight: {shape: [3072, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.3.mlp.fc2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.3.mlp.fc2.weight: {shape: [768, 3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.3.self_attn.k_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.3.self_attn.k_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.3.self_attn.out_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.3.self_attn.out_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.3.self_attn.q_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.3.self_attn.q_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.3.self_attn.v_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.3.self_attn.v_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.4.layer_norm1.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.4.layer_norm1.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.4.layer_norm2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.4.layer_norm2.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.4.mlp.fc1.bias: {shape: [3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.4.mlp.fc1.weight: {shape: [3072, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.4.mlp.fc2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.4.mlp.fc2.weight: {shape: [768, 3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.4.self_attn.k_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.4.self_attn.k_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.4.self_attn.out_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.4.self_attn.out_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.4.self_attn.q_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.4.self_attn.q_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.4.self_attn.v_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.4.self_attn.v_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.5.layer_norm1.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.5.layer_norm1.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.5.layer_norm2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.5.layer_norm2.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.5.mlp.fc1.bias: {shape: [3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.5.mlp.fc1.weight: {shape: [3072, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.5.mlp.fc2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.5.mlp.fc2.weight: {shape: [768, 3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.5.self_attn.k_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.5.self_attn.k_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.5.self_attn.out_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.5.self_attn.out_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.5.self_attn.q_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.5.self_attn.q_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.5.self_attn.v_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.5.self_attn.v_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.6.layer_norm1.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.6.layer_norm1.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.6.layer_norm2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.6.layer_norm2.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.6.mlp.fc1.bias: {shape: [3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.6.mlp.fc1.weight: {shape: [3072, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.6.mlp.fc2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.6.mlp.fc2.weight: {shape: [768, 3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.6.self_attn.k_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.6.self_attn.k_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.6.self_attn.out_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.6.self_attn.out_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.6.self_attn.q_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.6.self_attn.q_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.6.self_attn.v_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.6.self_attn.v_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.7.layer_norm1.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.7.layer_norm1.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.7.layer_norm2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.7.layer_norm2.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.7.mlp.fc1.bias: {shape: [3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.7.mlp.fc1.weight: {shape: [3072, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.7.mlp.fc2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.7.mlp.fc2.weight: {shape: [768, 3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.7.self_attn.k_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.7.self_attn.k_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.7.self_attn.out_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.7.self_attn.out_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.7.self_attn.q_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.7.self_attn.q_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.7.self_attn.v_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.7.self_attn.v_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.8.layer_norm1.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.8.layer_norm1.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.8.layer_norm2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.8.layer_norm2.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.8.mlp.fc1.bias: {shape: [3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.8.mlp.fc1.weight: {shape: [3072, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.8.mlp.fc2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.8.mlp.fc2.weight: {shape: [768, 3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.8.self_attn.k_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.8.self_attn.k_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.8.self_attn.out_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.8.self_attn.out_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.8.self_attn.q_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.8.self_attn.q_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.8.self_attn.v_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.8.self_attn.v_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.9.layer_norm1.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.9.layer_norm1.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.9.layer_norm2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.9.layer_norm2.weight: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.9.mlp.fc1.bias: {shape: [3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.9.mlp.fc1.weight: {shape: [3072, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.9.mlp.fc2.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.9.mlp.fc2.weight: {shape: [768, 3072], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.9.self_attn.k_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.9.self_attn.k_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.9.self_attn.out_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.9.self_attn.out_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.9.self_attn.q_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.9.self_attn.q_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.9.self_attn.v_proj.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.encoder.layers.9.self_attn.v_proj.weight: {shape: [768, 768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.final_layer_norm.bias: {shape: [768], dtype: float32}
    text_encoders.clip_l.transformer.text_model.final_layer_norm.weight: {shape: [768], dtype: float32}
  t5xxl:
    text_encoders.t5xxl.transformer.encoder.block.0.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.0.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.0.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: {shape: [32, 64], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.0.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.0.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.0.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.0.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.0.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.0.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.1.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.1.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.1.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.1.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.1.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.1.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.1.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.1.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.1.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.10.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.10.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.10.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.10.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.10.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.10.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.10.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.10.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.10.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.11.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.11.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.11.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.11.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.11.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.11.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.11.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.11.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.11.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.12.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.12.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.12.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.12.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.12.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.12.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.12.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.12.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.12.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.13.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.13.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.13.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.13.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.13.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.13.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.13.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.13.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.13.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.14.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.14.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.14.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.14.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.14.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.14.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.14.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.14.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.14.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.15.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.15.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.15.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.15.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.15.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.15.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.15.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.15.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.15.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.16.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.16.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.16.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.16.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.16.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.16.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.16.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.16.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.16.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.17.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.17.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.17.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.17.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.17.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.17.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.17.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.17.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.17.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.18.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.18.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.18.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.18.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.18.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.18.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.18.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.18.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.18.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.19.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.19.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.19.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.19.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.19.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.19.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.19.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.19.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.19.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.2.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.2.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.2.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.2.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.2.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.2.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.2.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.2.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.2.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.20.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.20.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.20.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.20.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.20.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.20.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.20.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.20.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.20.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.21.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.21.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.21.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.21.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.21.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.21.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.21.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.21.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.21.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.22.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.22.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.22.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.22.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.22.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.22.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.22.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.22.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.22.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.23.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.23.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.23.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.23.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.23.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.23.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.23.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.23.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.23.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.3.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.3.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.3.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.3.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.3.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.3.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.3.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.3.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.3.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.4.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.4.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.4.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.4.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.4.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.4.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.4.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.4.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.4.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.5.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.5.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.5.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.5.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.5.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.5.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.5.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.5.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.5.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.6.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.6.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.6.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.6.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.6.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.6.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.6.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.6.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.6.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.7.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.7.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.7.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.7.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.7.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.7.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.7.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.7.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.7.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.8.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.8.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.8.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.8.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.8.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.8.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.8.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.8.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.8.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.9.layer.0.SelfAttention.k.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.9.layer.0.SelfAttention.o.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.9.layer.0.SelfAttention.q.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.9.layer.0.SelfAttention.v.weight: {shape: [4096, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.9.layer.0.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.9.layer.1.DenseReluDense.wi_0.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.9.layer.1.DenseReluDense.wi_1.weight: {shape: [10240, 4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.9.layer.1.DenseReluDense.wo.weight: {shape: [4096, 10240], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.block.9.layer.1.layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.encoder.final_layer_norm.weight: {shape: [4096], dtype: float32}
    text_encoders.t5xxl.transformer.shared.weight: {shape: [32128, 4096], dtype: float32}
  vae:
    vae.decoder.conv_in.bias: {shape: [512], dtype: float32}
    vae.decoder.conv_in.weight: {shape: [512, 16, 3, 3], dtype: float32}
    vae.decoder.conv_out.bias: {shape: [3], dtype: float32}
    vae.decoder.conv_out.weight: {shape: [3, 128, 3, 3], dtype: float32}
    vae.decoder.mid.attn_1.k.bias: {shape: [512], dtype: float32}
    vae.decoder.mid.attn_1.k.weight: {shape: [512, 512, 1, 1], dtype: float32}
    vae.decoder.mid.attn_1.norm.bias: {shape: [512], dtype: float32}
    vae.decoder.mid.attn_1.norm.weight: {shape: [512], dtype: float32}
    vae.decoder.mid.attn_1.proj_out.bias: {shape: [512], dtype: float32}
    vae.decoder.mid.attn_1.proj_out.weight: {shape: [512, 512, 1, 1], dtype: float32}
    vae.decoder.mid.attn_1.q.bias: {shape: [512], dtype: float32}
    vae.decoder.mid.attn_1.q.weight: {shape: [512, 512, 1, 1], dtype: float32}
    vae.decoder.mid.attn_1.v.bias: {shape: [512], dtype: float32}
    vae.decoder.mid.attn_1.v.weight: {shape: [512, 512, 1, 1], dtype: float32}
    vae.decoder.mid.block_1.conv1.bias: {shape: [512], dtype: float32}
    vae.decoder.mid.block_1.conv1.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.decoder.mid.block_1.conv2.bias: {shape: [512], dtype: float32}
    vae.decoder.mid.block_1.conv2.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.decoder.mid.block_1.norm1.bias: {shape: [512], dtype: float32}
    vae.decoder.mid.block_1.norm1.weight: {shape: [512], dtype: float32}
    vae.decoder.mid.block_1.norm2.bias: {shape: [512], dtype: float32}
    vae.decoder.mid.block_1.norm2.weight: {shape: [512], dtype: float32}
    vae.decoder.mid.block_2.conv1.bias: {shape: [512], dtype: float32}
    vae.decoder.mid.block_2.conv1.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.decoder.mid.block_2.conv2.bias: {shape: [512], dtype: float32}
    vae.decoder.mid.block_2.conv2.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.decoder.mid.block_2.norm1.bias: {shape: [512], dtype: float32}
    vae.decoder.mid.block_2.norm1.weight: {shape: [512], dtype: float32}
    vae.decoder.mid.block_2.norm2.bias: {shape: [512], dtype: float32}
    vae.decoder.mid.block_2.norm2.weight: {shape: [512], dtype: float32}
    vae.decoder.norm_out.bias: {shape: [128], dtype: float32}
    vae.decoder.norm_out.weight: {shape: [128], dtype: float32}
    vae.decoder.up.0.block.0.conv1.bias: {shape: [128], dtype: float32}
    vae.decoder.up.0.block.0.conv1.weight: {shape: [128, 256, 3, 3], dtype: float32}
    vae.decoder.up.0.block.0.conv2.bias: {shape: [128], dtype: float32}
    vae.decoder.up.0.block.0.conv2.weight: {shape: [128, 128, 3, 3], dtype: float32}
    vae.decoder.up.0.block.0.nin_shortcut.bias: {shape: [128], dtype: float32}
    vae.decoder.up.0.block.0.nin_shortcut.weight: {shape: [128, 256, 1, 1], dtype: float32}
    vae.decoder.up.0.block.0.norm1.bias: {shape: [256], dtype: float32}
    vae.decoder.up.0.block.0.norm1.weight: {shape: [256], dtype: float32}
    vae.decoder.up.0.block.0.norm2.bias: {shape: [128], dtype: float32}
    vae.decoder.up.0.block.0.norm2.weight: {shape: [128], dtype: float32}
    vae.decoder.up.0.block.1.conv1.bias: {shape: [128], dtype: float32}
    vae.decoder.up.0.block.1.conv1.weight: {shape: [128, 128, 3, 3], dtype: float32}
    vae.decoder.up.0.block.1.conv2.bias: {shape: [128], dtype: float32}
    vae.decoder.up.0.block.1.conv2.weight: {shape: [128, 128, 3, 3], dtype: float32}
    vae.decoder.up.0.block.1.norm1.bias: {shape: [128], dtype: float32}
    vae.decoder.up.0.block.1.norm1.weight: {shape: [128], dtype: float32}
    vae.decoder.up.0.block.1.norm2.bias: {shape: [128], dtype: float32}
    vae.decoder.up.0.block.1.norm2.weight: {shape: [128], dtype: float32}
    vae.decoder.up.0.block.2.conv1.bias: {shape: [128], dtype: float32}
    vae.decoder.up.0.block.2.conv1.weight: {shape: [128, 128, 3, 3], dtype: float32}
    vae.decoder.up.0.block.2.conv2.bias: {shape: [128], dtype: float32}
    vae.decoder.up.0.block.2.conv2.weight: {shape: [128, 128, 3, 3], dtype: float32}
    vae.decoder.up.0.block.2.norm1.bias: {shape: [128], dtype: float32}
    vae.decoder.up.0.block.2.norm1.weight: {shape: [128], dtype: float32}
    vae.decoder.up.0.block.2.norm2.bias: {shape: [128], dtype: float32}
    vae.decoder.up.0.block.2.norm2.weight: {shape: [128], dtype: float32}
    vae.decoder.up.1.block.0.conv1.bias: {shape: [256], dtype: float32}
    vae.decoder.up.1.block.0.conv1.weight: {shape: [256, 512, 3, 3], dtype: float32}
    vae.decoder.up.1.block.0.conv2.bias: {shape: [256], dtype: float32}
    vae.decoder.up.1.block.0.conv2.weight: {shape: [256, 256, 3, 3], dtype: float32}
    vae.decoder.up.1.block.0.nin_shortcut.bias: {shape: [256], dtype: float32}
    vae.decoder.up.1.block.0.nin_shortcut.weight: {shape: [256, 512, 1, 1], dtype: float32}
    vae.decoder.up.1.block.0.norm1.bias: {shape: [512], dtype: float32}
    vae.decoder.up.1.block.0.norm1.weight: {shape: [512], dtype: float32}
    vae.decoder.up.1.block.0.norm2.bias: {shape: [256], dtype: float32}
    vae.decoder.up.1.block.0.norm2.weight: {shape: [256], dtype: float32}
    vae.decoder.up.1.block.1.conv1.bias: {shape: [256], dtype: float32}
    vae.decoder.up.1.block.1.conv1.weight: {shape: [256, 256, 3, 3], dtype: float32}
    vae.decoder.up.1.block.1.conv2.bias: {shape: [256], dtype: float32}
    vae.decoder.up.1.block.1.conv2.weight: {shape: [256, 256, 3, 3], dtype: float32}
    vae.decoder.up.1.block.1.norm1.bias: {shape: [256], dtype: float32}
    vae.decoder.up.1.block.1.norm1.weight: {shape: [256], dtype: float32}
    vae.decoder.up.1.block.1.norm2.bias: {shape: [256], dtype: float32}
    vae.decoder.up.1.block.1.norm2.weight: {shape: [256], dtype: float32}
    vae.decoder.up.1.block.2.conv1.bias: {shape: [256], dtype: float32}
    vae.decoder.up.1.block.2.conv1.weight: {shape: [256, 256, 3, 3], dtype: float32}
    vae.decoder.up.1.block.2.conv2.bias: {shape: [256], dtype: float32}
    vae.decoder.up.1.block.2.conv2.weight: {shape: [256, 256, 3, 3], dtype: float32}
    vae.decoder.up.1.block.2.norm1.bias: {shape: [256], dtype: float32}
    vae.decoder.up.1.block.2.norm1.weight: {shape: [256], dtype: float32}
    vae.decoder.up.1.block.2.norm2.bias: {shape: [256], dtype: float32}
    vae.decoder.up.1.block.2.norm2.weight: {shape: [256], dtype: float32}
    vae.decoder.up.1.upsample.conv.bias: {shape: [256], dtype: float32}
    vae.decoder.up.1.upsample.conv.weight: {shape: [256, 256, 3, 3], dtype: float32}
    vae.decoder.up.2.block.0.conv1.bias: {shape: [512], dtype: float32}
    vae.decoder.up.2.block.0.conv1.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.decoder.up.2.block.0.conv2.bias: {shape: [512], dtype: float32}
    vae.decoder.up.2.block.0.conv2.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.decoder.up.2.block.0.norm1.bias: {shape: [512], dtype: float32}
    vae.decoder.up.2.block.0.norm1.weight: {shape: [512], dtype: float32}
    vae.decoder.up.2.block.0.norm2.bias: {shape: [512], dtype: float32}
    vae.decoder.up.2.block.0.norm2.weight: {shape: [512], dtype: float32}
    vae.decoder.up.2.block.1.conv1.bias: {shape: [512], dtype: float32}
    vae.decoder.up.2.block.1.conv1.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.decoder.up.2.block.1.conv2.bias: {shape: [512], dtype: float32}
    vae.decoder.up.2.block.1.conv2.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.decoder.up.2.block.1.norm1.bias: {shape: [512], dtype: float32}
    vae.decoder.up.2.block.1.norm1.weight: {shape: [512], dtype: float32}
    vae.decoder.up.2.block.1.norm2.bias: {shape: [512], dtype: float32}
    vae.decoder.up.2.block.1.norm2.weight: {shape: [512], dtype: float32}
    vae.decoder.up.2.block.2.conv1.bias: {shape: [512], dtype: float32}
    vae.decoder.up.2.block.2.conv1.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.decoder.up.2.block.2.conv2.bias: {shape: [512], dtype: float32}
    vae.decoder.up.2.block.2.conv2.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.decoder.up.2.block.2.norm1.bias: {shape: [512], dtype: float32}
    vae.decoder.up.2.block.2.norm1.weight: {shape: [512], dtype: float32}
    vae.decoder.up.2.block.2.norm2.bias: {shape: [512], dtype: float32}
    vae.decoder.up.2.block.2.norm2.weight: {shape: [512], dtype: float32}
    vae.decoder.up.2.upsample.conv.bias: {shape: [512], dtype: float32}
    vae.decoder.up.2.upsample.conv.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.decoder.up.3.block.0.conv1.bias: {shape: [512], dtype: float32}
    vae.decoder.up.3.block.0.conv1.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.decoder.up.3.block.0.conv2.bias: {shape: [512], dtype: float32}
    vae.decoder.up.3.block.0.conv2.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.decoder.up.3.block.0.norm1.bias: {shape: [512], dtype: float32}
    vae.decoder.up.3.block.0.norm1.weight: {shape: [512], dtype: float32}
    vae.decoder.up.3.block.0.norm2.bias: {shape: [512], dtype: float32}
    vae.decoder.up.3.block.0.norm2.weight: {shape: [512], dtype: float32}
    vae.decoder.up.3.block.1.conv1.bias: {shape: [512], dtype: float32}
    vae.decoder.up.3.block.1.conv1.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.decoder.up.3.block.1.conv2.bias: {shape: [512], dtype: float32}
    vae.decoder.up.3.block.1.conv2.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.decoder.up.3.block.1.norm1.bias: {shape: [512], dtype: float32}
    vae.decoder.up.3.block.1.norm1.weight: {shape: [512], dtype: float32}
    vae.decoder.up.3.block.1.norm2.bias: {shape: [512], dtype: float32}
    vae.decoder.up.3.block.1.norm2.weight: {shape: [512], dtype: float32}
    vae.decoder.up.3.block.2.conv1.bias: {shape: [512], dtype: float32}
    vae.decoder.up.3.block.2.conv1.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.decoder.up.3.block.2.conv2.bias: {shape: [512], dtype: float32}
    vae.decoder.up.3.block.2.conv2.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.decoder.up.3.block.2.norm1.bias: {shape: [512], dtype: float32}
    vae.decoder.up.3.block.2.norm1.weight: {shape: [512], dtype: float32}
    vae.decoder.up.3.block.2.norm2.bias: {shape: [512], dtype: float32}
    vae.decoder.up.3.block.2.norm2.weight: {shape: [512], dtype: float32}
    vae.decoder.up.3.upsample.conv.bias: {shape: [512], dtype: float32}
    vae.decoder.up.3.upsample.conv.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.encoder.conv_in.bias: {shape: [128], dtype: float32}
    vae.encoder.conv_in.weight: {shape: [128, 3, 3, 3], dtype: float32}
    vae.encoder.conv_out.bias: {shape: [32], dtype: float32}
    vae.encoder.conv_out.weight: {shape: [32, 512, 3, 3], dtype: float32}
    vae.encoder.down.0.block.0.conv1.bias: {shape: [128], dtype: float32}
    vae.encoder.down.0.block.0.conv1.weight: {shape: [128, 128, 3, 3], dtype: float32}
    vae.encoder.down.0.block.0.conv2.bias: {shape: [128], dtype: float32}
    vae.encoder.down.0.block.0.conv2.weight: {shape: [128, 128, 3, 3], dtype: float32}
    vae.encoder.down.0.block.0.norm1.bias: {shape: [128], dtype: float32}
    vae.encoder.down.0.block.0.norm1.weight: {shape: [128], dtype: float32}
    vae.encoder.down.0.block.0.norm2.bias: {shape: [128], dtype: float32}
    vae.encoder.down.0.block.0.norm2.weight: {shape: [128], dtype: float32}
    vae.encoder.down.0.block.1.conv1.bias: {shape: [128], dtype: float32}
    vae.encoder.down.0.block.1.conv1.weight: {shape: [128, 128, 3, 3], dtype: float32}
    vae.encoder.down.0.block.1.conv2.bias: {shape: [128], dtype: float32}
    vae.encoder.down.0.block.1.conv2.weight: {shape: [128, 128, 3, 3], dtype: float32}
    vae.encoder.down.0.block.1.norm1.bias: {shape: [128], dtype: float32}
    vae.encoder.down.0.block.1.norm1.weight: {shape: [128], dtype: float32}
    vae.encoder.down.0.block.1.norm2.bias: {shape: [128], dtype: float32}
    vae.encoder.down.0.block.1.norm2.weight: {shape: [128], dtype: float32}
    vae.encoder.down.0.downsample.conv.bias: {shape: [128], dtype: float32}
    vae.encoder.down.0.downsample.conv.weight: {shape: [128, 128, 3, 3], dtype: float32}
    vae.encoder.down.1.block.0.conv1.bias: {shape: [256], dtype: float32}
    vae.encoder.down.1.block.0.conv1.weight: {shape: [256, 128, 3, 3], dtype: float32}
    vae.encoder.down.1.block.0.conv2.bias: {shape: [256], dtype: float32}
    vae.encoder.down.1.block.0.conv2.weight: {shape: [256, 256, 3, 3], dtype: float32}
    vae.encoder.down.1.block.0.nin_shortcut.bias: {shape: [256], dtype: float32}
    vae.encoder.down.1.block.0.nin_shortcut.weight: {shape: [256, 128, 1, 1], dtype: float32}
    vae.encoder.down.1.block.0.norm1.bias: {shape: [128], dtype: float32}
    vae.encoder.down.1.block.0.norm1.weight: {shape: [128], dtype: float32}
    vae.encoder.down.1.block.0.norm2.bias: {shape: [256], dtype: float32}
    vae.encoder.down.1.block.0.norm2.weight: {shape: [256], dtype: float32}
    vae.encoder.down.1.block.1.conv1.bias: {shape: [256], dtype: float32}
    vae.encoder.down.1.block.1.conv1.weight: {shape: [256, 256, 3, 3], dtype: float32}
    vae.encoder.down.1.block.1.conv2.bias: {shape: [256], dtype: float32}
    vae.encoder.down.1.block.1.conv2.weight: {shape: [256, 256, 3, 3], dtype: float32}
    vae.encoder.down.1.block.1.norm1.bias: {shape: [256], dtype: float32}
    vae.encoder.down.1.block.1.norm1.weight: {shape: [256], dtype: float32}
    vae.encoder.down.1.block.1.norm2.bias: {shape: [256], dtype: float32}
    vae.encoder.down.1.block.1.norm2.weight: {shape: [256], dtype: float32}
    vae.encoder.down.1.downsample.conv.bias: {shape: [256], dtype: float32}
    vae.encoder.down.1.downsample.conv.weight: {shape: [256, 256, 3, 3], dtype: float32}
    vae.encoder.down.2.block.0.conv1.bias: {shape: [512], dtype: float32}
    vae.encoder.down.2.block.0.conv1.weight: {shape: [512, 256, 3, 3], dtype: float32}
    vae.encoder.down.2.block.0.conv2.bias: {shape: [512], dtype: float32}
    vae.encoder.down.2.block.0.conv2.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.encoder.down.2.block.0.nin_shortcut.bias: {shape: [512], dtype: float32}
    vae.encoder.down.2.block.0.nin_shortcut.weight: {shape: [512, 256, 1, 1], dtype: float32}
    vae.encoder.down.2.block.0.norm1.bias: {shape: [256], dtype: float32}
    vae.encoder.down.2.block.0.norm1.weight: {shape: [256], dtype: float32}
    vae.encoder.down.2.block.0.norm2.bias: {shape: [512], dtype: float32}
    vae.encoder.down.2.block.0.norm2.weight: {shape: [512], dtype: float32}
    vae.encoder.down.2.block.1.conv1.bias: {shape: [512], dtype: float32}
    vae.encoder.down.2.block.1.conv1.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.encoder.down.2.block.1.conv2.bias: {shape: [512], dtype: float32}
    vae.encoder.down.2.block.1.conv2.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.encoder.down.2.block.1.norm1.bias: {shape: [512], dtype: float32}
    vae.encoder.down.2.block.1.norm1.weight: {shape: [512], dtype: float32}
    vae.encoder.down.2.block.1.norm2.bias: {shape: [512], dtype: float32}
    vae.encoder.down.2.block.1.norm2.weight: {shape: [512], dtype: float32}
    vae.encoder.down.2.downsample.conv.bias: {shape: [512], dtype: float32}
    vae.encoder.down.2.downsample.conv.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.encoder.down.3.block.0.conv1.bias: {shape: [512], dtype: float32}
    vae.encoder.down.3.block.0.conv1.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.encoder.down.3.block.0.conv2.bias: {shape: [512], dtype: float32}
    vae.encoder.down.3.block.0.conv2.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.encoder.down.3.block.0.norm1.bias: {shape: [512], dtype: float32}
    vae.encoder.down.3.block.0.norm1.weight: {shape: [512], dtype: float32}
    vae.encoder.down.3.block.0.norm2.bias: {shape: [512], dtype: float32}
    vae.encoder.down.3.block.0.norm2.weight: {shape: [512], dtype: float32}
    vae.encoder.down.3.block.1.conv1.bias: {shape: [512], dtype: float32}
    vae.encoder.down.3.block.1.conv1.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.encoder.down.3.block.1.conv2.bias: {shape: [512], dtype: float32}
    vae.encoder.down.3.block.1.conv2.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.encoder.down.3.block.1.norm1.bias: {shape: [512], dtype: float32}
    vae.encoder.down.3.block.1.norm1.weight: {shape: [512], dtype: float32}
    vae.encoder.down.3.block.1.norm2.bias: {shape: [512], dtype: float32}
    vae.encoder.down.3.block.1.norm2.weight: {shape: [512], dtype: float32}
    vae.encoder.mid.attn_1.k.bias: {shape: [512], dtype: float32}
    vae.encoder.mid.attn_1.k.weight: {shape: [512, 512, 1, 1], dtype: float32}
    vae.encoder.mid.attn_1.norm.bias: {shape: [512], dtype: float32}
    vae.encoder.mid.attn_1.norm.weight: {shape: [512], dtype: float32}
    vae.encoder.mid.attn_1.proj_out.bias: {shape: [512], dtype: float32}
    vae.encoder.mid.attn_1.proj_out.weight: {shape: [512, 512, 1, 1], dtype: float32}
    vae.encoder.mid.attn_1.q.bias: {shape: [512], dtype: float32}
    vae.encoder.mid.attn_1.q.weight: {shape: [512, 512, 1, 1], dtype: float32}
    vae.encoder.mid.attn_1.v.bias: {shape: [512], dtype: float32}
    vae.encoder.mid.attn_1.v.weight: {shape: [512, 512, 1, 1], dtype: float32}
    vae.encoder.mid.block_1.conv1.bias: {shape: [512], dtype: float32}
    vae.encoder.mid.block_1.conv1.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.encoder.mid.block_1.conv2.bias: {shape: [512], dtype: float32}
    vae.encoder.mid.block_1.conv2.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.encoder.mid.block_1.norm1.bias: {shape: [512], dtype: float32}
    vae.encoder.mid.block_1.norm1.weight: {shape: [512], dtype: float32}
    vae.encoder.mid.block_1.norm2.bias: {shape: [512], dtype: float32}
    vae.encoder.mid.block_1.norm2.weight: {shape: [512], dtype: float32}
    vae.encoder.mid.block_2.conv1.bias: {shape: [512], dtype: float32}
    vae.encoder.mid.block_2.conv1.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.encoder.mid.block_2.conv2.bias: {shape: [512], dtype: float32}
    vae.encoder.mid.block_2.conv2.weight: {shape: [512, 512, 3, 3], dtype: float32}
    vae.encoder.mid.block_2.norm1.bias: {shape: [512], dtype: float32}
    vae.encoder.mid.block_2.norm1.weight: {shape: [512], dtype: float32}
    vae.encoder.mid.block_2.norm2.bias: {shape: [512], dtype: float32}
    vae.encoder.mid.block_2.norm2.weight: {shape: [512], dtype: float32}
    vae.encoder.norm_out.bias: {shape: [512], dtype: float32}
    vae.encoder.norm_out.weight: {shape: [512], dtype: float32}
